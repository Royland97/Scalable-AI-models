{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "axcFL8w9_kJ-",
        "outputId": "738dfb6d-cc83-43ad-8fc0-36aba42a2076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jdk is already the newest version (11.0.28+6-1ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWQsG5OR1lEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a2f129-0974-4db4-df30-35beb494ad31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark: 3.5.1\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#Crear una sesión de Spark\n",
        "spark = (SparkSession.builder\n",
        "    .appName('Arbol Decision Escalable')\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate())\n",
        "\n",
        "spark.sparkContext.setLogLevel('WARN')\n",
        "\n",
        "print(\"Spark: \"+spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "BHmvivCs2vL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, 0, 2, 'rayado', 'normal'),\n",
        "    (1, 0, 1, 'blanco', 'cancerigena'),\n",
        "    (1, 2, 0, 'rayado', 'normal'),\n",
        "    (0, 2, 1, 'rayado', 'normal'),\n",
        "    (1, 1, 1, 'rayado', 'cancerigena'),\n",
        "    (2, 2, 1, 'rayado', 'cancerigena')\n",
        "]\n",
        "\n",
        "columns = ['antenas','colas','nucleos','cuerpo','clase']\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dpH59L-2xjc",
        "outputId": "dcd7d43c-1086-4595-8d56-5e6b01e4d969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-------+------+-----------+\n",
            "|antenas|colas|nucleos|cuerpo|      clase|\n",
            "+-------+-----+-------+------+-----------+\n",
            "|      1|    0|      2|rayado|     normal|\n",
            "|      1|    0|      1|blanco|cancerigena|\n",
            "|      1|    2|      0|rayado|     normal|\n",
            "|      0|    2|      1|rayado|     normal|\n",
            "|      1|    1|      1|rayado|cancerigena|\n",
            "|      2|    2|      1|rayado|cancerigena|\n",
            "+-------+-----+-------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculo de Entropia MapReduce"
      ],
      "metadata": {
        "id": "kx2ylauJ_yHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def entropia(df_columna):\n",
        "    n_total = df_columna.count()\n",
        "    if n_total == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Convertimos a RDD y contamos ocurrencias por valor\n",
        "    rdd = df_columna.rdd.map(lambda row: (row[0], 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    def calcular_entropia_particion(iterator):\n",
        "        resultados = []\n",
        "        for valor, count in iterator:\n",
        "            p = count / n_total\n",
        "            resultados.append(-(p * np.log2(p)))\n",
        "        return resultados\n",
        "\n",
        "    entropia_valores = rdd.mapPartitions(calcular_entropia_particion)\n",
        "\n",
        "    # Reduce: sumar todas los valores\n",
        "    I = entropia_valores.reduce(lambda a, b: a + b)\n",
        "    return I\n",
        "\n",
        "def entropia_atributo(df, atributo, clase_col):\n",
        "    n_total = df.count()\n",
        "    if n_total == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Obtener los valores únicos del atributo\n",
        "    valores = [row[atributo] for row in df.select(atributo).distinct().collect()]\n",
        "\n",
        "    I_Ai = 0.0\n",
        "    for valor in valores:\n",
        "        grupo = df.filter(col(atributo) == valor)\n",
        "        n_ij = grupo.count()\n",
        "        I_ij = entropia(grupo.select(clase_col))\n",
        "        I_Ai += (n_ij / n_total) * I_ij\n",
        "    return I_Ai\n",
        "\n",
        "def entropia_ganancia(df, atributo, clase_col):\n",
        "    return entropia(df.select(clase_col)) - entropia_atributo(df, atributo, clase_col)\n",
        "\n",
        "# entropia_nucleos = entropia_atributo(df, 'cuerpo', 'clase')\n",
        "# print(f\"Entropía condicional del atributo 'cuerpo': {entropia_nucleos:.4f}\")"
      ],
      "metadata": {
        "id": "soGQKDVdywMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID3 Secuencial"
      ],
      "metadata": {
        "id": "wJjA3bO904p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "def id3_secuencial(df, atributos, clase_col):\n",
        "    arbol = {}\n",
        "    pendientes = [(df, atributos, arbol, None)]\n",
        "\n",
        "    while pendientes:\n",
        "        df_sub, attrs, nodo_padre, clave_padre = pendientes.pop()\n",
        "\n",
        "        # Entropía total en este nodo\n",
        "        clases_unicas = [r[clase_col] for r in df_sub.select(clase_col).distinct().collect()]\n",
        "        if len(clases_unicas) == 1:\n",
        "            if clave_padre is None:\n",
        "                return clases_unicas[0]\n",
        "            else:\n",
        "                nodo_padre[clave_padre] = clases_unicas[0]\n",
        "            continue\n",
        "\n",
        "        # Calcular ganancia para cada atributo\n",
        "        ganancias = {a: entropia_ganancia(df_sub, a, clase_col) for a in attrs}\n",
        "        atributo_ganador = max(ganancias, key=ganancias.get)\n",
        "\n",
        "        # Crear nodo para este atributo en el padre\n",
        "        nuevo_nodo = {}\n",
        "        if clave_padre is None:\n",
        "            nodo_padre.update({atributo_ganador: nuevo_nodo})\n",
        "        else:\n",
        "            nodo_padre[clave_padre] = {atributo_ganador: nuevo_nodo}\n",
        "\n",
        "        # Para cada valor posible del atributo ganador, añadir nuevos nodos pendientes\n",
        "        valores = [row[atributo_ganador] for row in df_sub.select(atributo_ganador).distinct().collect()]\n",
        "        for val in valores:\n",
        "            sub_df = df_sub.filter(col(atributo_ganador) == val)\n",
        "            if sub_df.count() == 0:\n",
        "                clase_mayoritaria = df_sub.groupBy(clase_col).count().orderBy(col('count').desc()).first()[0]\n",
        "                nuevo_nodo[val] = clase_mayoritaria\n",
        "            else:\n",
        "                # Agregar nodo pendiente a la pila\n",
        "                sub_attrs = [a for a in attrs if a != atributo_ganador]\n",
        "                pendientes.append((sub_df, sub_attrs, nuevo_nodo, val))\n",
        "\n",
        "    return arbol"
      ],
      "metadata": {
        "id": "Rrt4-wvqMFqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultado"
      ],
      "metadata": {
        "id": "dMN9AR8-OZW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "arbol = id3_secuencial(df, df.columns[:-1], 'clase')\n",
        "print(json.dumps(arbol, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pibLk34rJpMn",
        "outputId": "e1234e45-ba9d-4044-9db2-3f42af582927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"nucleos\": {\n",
            "        \"2\": \"normal\",\n",
            "        \"1\": {\n",
            "            \"antenas\": {\n",
            "                \"2\": \"cancerigena\",\n",
            "                \"0\": \"normal\",\n",
            "                \"1\": \"cancerigena\"\n",
            "            }\n",
            "        },\n",
            "        \"0\": \"normal\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}